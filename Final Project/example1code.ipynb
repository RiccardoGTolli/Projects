{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the webpage\n",
    "import requests\n",
    "req= requests.get (\"https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the HTML file with BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "bsoup= BeautifulSoup(req.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating each sentence\n",
    "sentences= bsoup.find_all(\"span\", attrs={\"class\":\"short-desc\"})\n",
    "# this searches all the soup objects with the \"span\" tag with attribute  class equal short-desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) # number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data is pretty untidy, need to make it tidy and usable by extracting :\n",
    "\n",
    "# 1. the date\n",
    "# 2. the lie\n",
    "# 3. the reason why it is a lie\n",
    "# 4. the hyperlink included in the reason\n",
    "\n",
    "# this is useful to create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jan. 21, 2017'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting date from the first object of sentences\n",
    "first_sentence= sentences[0]\n",
    "first_sentence.find(\"strong\").text[0:-1] + \", 2017\"  # extracting date from tag \"strong\" and adding year manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I wasn't a fan of Iraq. I didn't want to go into Iraq.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting the lie from the first object of sentences\n",
    "first_sentence.contents # this returns a list of the children in the html code (tags and strings nested within a tag)\n",
    "first_sentence.contents[1] [1:-2] # takes only the second element of the list and within it, removing the excessive quotation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was for an invasion before he was against it.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting the reason from the first object of sentences\n",
    "first_sentence.find(\"a\").text [1:-1] # extracting tag \"a\" and removing parenthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.buzzfeed.com/andrewkaczynski/in-2002-donald-trump-said-he-supported-invading-iraq-on-the'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting the hyperlink from the first object of sentences\n",
    "first_sentence.find(\"a\")[\"href\"] # accessing the \"href\" attribute within the \"a\" tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all the sentences and extract the items above\n",
    "\n",
    "all_items=[]\n",
    "for sentence in sentences:\n",
    "    date= sentence.find(\"strong\").text[0:-1] + \", 2017\" \n",
    "    lie= sentence.contents [1][1:-2]\n",
    "    reason= sentence.find (\"a\").text[1:-1]\n",
    "    hyperlink= sentence.find (\"a\")[\"href\"]\n",
    "    all_items.append((date,lie,reason,hyperlink))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_items) # a list of 180 objects (1 per sentence) of tuples of length 4 with date, lie, reason and hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "import pandas as pd\n",
    "df= pd.DataFrame(all_items, columns=[\"date\",\"lie\",\"reason\",\"hyperlink\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lie</th>\n",
       "      <th>reason</th>\n",
       "      <th>hyperlink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan. 21, 2017</td>\n",
       "      <td>I wasn't a fan of Iraq. I didn't want to go in...</td>\n",
       "      <td>He was for an invasion before he was against it.</td>\n",
       "      <td>https://www.buzzfeed.com/andrewkaczynski/in-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan. 21, 2017</td>\n",
       "      <td>A reporter for Time magazine — and I have been...</td>\n",
       "      <td>Trump was on the cover 11 times and Nixon appe...</td>\n",
       "      <td>http://nation.time.com/2013/11/06/10-things-yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan. 23, 2017</td>\n",
       "      <td>Between 3 million and 5 million illegal votes ...</td>\n",
       "      <td>There's no evidence of illegal voting.</td>\n",
       "      <td>https://www.nytimes.com/2017/01/23/us/politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan. 25, 2017</td>\n",
       "      <td>Now, the audience was the biggest ever. But th...</td>\n",
       "      <td>Official aerial photos show Obama's 2009 inaug...</td>\n",
       "      <td>https://www.nytimes.com/2017/01/21/us/politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan. 25, 2017</td>\n",
       "      <td>Take a look at the Pew reports (which show vot...</td>\n",
       "      <td>The report never mentioned voter fraud.</td>\n",
       "      <td>https://www.nytimes.com/2017/01/24/us/politics...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                                lie  \\\n",
       "0  Jan. 21, 2017  I wasn't a fan of Iraq. I didn't want to go in...   \n",
       "1  Jan. 21, 2017  A reporter for Time magazine — and I have been...   \n",
       "2  Jan. 23, 2017  Between 3 million and 5 million illegal votes ...   \n",
       "3  Jan. 25, 2017  Now, the audience was the biggest ever. But th...   \n",
       "4  Jan. 25, 2017  Take a look at the Pew reports (which show vot...   \n",
       "\n",
       "                                              reason  \\\n",
       "0   He was for an invasion before he was against it.   \n",
       "1  Trump was on the cover 11 times and Nixon appe...   \n",
       "2             There's no evidence of illegal voting.   \n",
       "3  Official aerial photos show Obama's 2009 inaug...   \n",
       "4            The report never mentioned voter fraud.   \n",
       "\n",
       "                                           hyperlink  \n",
       "0  https://www.buzzfeed.com/andrewkaczynski/in-20...  \n",
       "1  http://nation.time.com/2013/11/06/10-things-yo...  \n",
       "2  https://www.nytimes.com/2017/01/23/us/politics...  \n",
       "3  https://www.nytimes.com/2017/01/21/us/politics...  \n",
       "4  https://www.nytimes.com/2017/01/24/us/politics...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the date format into pandas format\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lie</th>\n",
       "      <th>reason</th>\n",
       "      <th>hyperlink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>We have trade deficits with almost everybody.</td>\n",
       "      <td>We have trade surpluses with more than 100 cou...</td>\n",
       "      <td>https://www.bea.gov/newsreleases/international...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>Wacky &amp; totally unhinged Tom Steyer, who has b...</td>\n",
       "      <td>Steyer has financially supported many winning ...</td>\n",
       "      <td>https://www.opensecrets.org/donor-lookup/resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>Again, we're the highest-taxed nation, just ab...</td>\n",
       "      <td>We're not.</td>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>When you look at the city with the strongest g...</td>\n",
       "      <td>Several other cities, including New York and L...</td>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>I'd rather have him  – you know, work with him...</td>\n",
       "      <td>There is no evidence that Democrats \"set up\" R...</td>\n",
       "      <td>https://www.nytimes.com/interactive/2017/12/10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                                lie  \\\n",
       "175 2017-10-25      We have trade deficits with almost everybody.   \n",
       "176 2017-10-27  Wacky & totally unhinged Tom Steyer, who has b...   \n",
       "177 2017-11-01  Again, we're the highest-taxed nation, just ab...   \n",
       "178 2017-11-07  When you look at the city with the strongest g...   \n",
       "179 2017-11-11  I'd rather have him  – you know, work with him...   \n",
       "\n",
       "                                                reason  \\\n",
       "175  We have trade surpluses with more than 100 cou...   \n",
       "176  Steyer has financially supported many winning ...   \n",
       "177                                         We're not.   \n",
       "178  Several other cities, including New York and L...   \n",
       "179  There is no evidence that Democrats \"set up\" R...   \n",
       "\n",
       "                                             hyperlink  \n",
       "175  https://www.bea.gov/newsreleases/international...  \n",
       "176  https://www.opensecrets.org/donor-lookup/resul...  \n",
       "177  http://www.politifact.com/truth-o-meter/statem...  \n",
       "178  http://www.politifact.com/truth-o-meter/statem...  \n",
       "179  https://www.nytimes.com/interactive/2017/12/10...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting dataset to csv for future possible analysis\n",
    "df.to_csv(\"example3code.csv\", index=False,encoding= \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c56f90e048>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of dates\n",
    "df_date=df[\"date\"] # store date in list\n",
    "df_date = df_date.astype(\"datetime64\")# convert to date type\n",
    "df_date.groupby(df_date.dt.month).count().plot(kind=\"bar\") # grouping by month and plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# which websites are the links from, how many auto references, how many of those websites are openly left wing\n",
    "# this is an effort to analyze if there is bias\n",
    "\n",
    "df_hyperlink=df[\"hyperlink\"] # store the links in a list\n",
    "\n",
    "# we need the source not the entire web address\n",
    "\n",
    "df_hyperlink=df_hyperlink.tolist() # make it a list of strings rather than pandas.series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let us remove \"http://\" and \"https://www.\" \n",
    "\n",
    "list_1 = [\"http://\",\"https://www.\",\"https://\",\"www.\"] # characters to remove\n",
    "accumulator_links=-1 # setting accumulator\n",
    "for link in df_hyperlink: # loop through links list\n",
    "    accumulator_links+=1  # add one to accumulator\n",
    "    for element in list_1: # loop through characters to remove\n",
    "        df_hyperlink[accumulator_links]= df_hyperlink[accumulator_links].replace(element,\"\") # remove those characters from the links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us remove anything after the first forward slash so to get only the main domains\n",
    "separator = '/' # setting a character after which I want to remove all text\n",
    "accumulator_links_1=0 # setting accumulator\n",
    "for link in df_hyperlink: # loop through the links \n",
    "    df_hyperlink[accumulator_links_1]=link.split(separator, 1)[0] # remove all text after the separator\n",
    "    accumulator_links_1+=1 # add one to accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyperlink = pd.Series(df_hyperlink) # re-convert to pandas series in order to use value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nytimes.com                57\n",
       "washingtonpost.com         39\n",
       "politifact.com             31\n",
       "factcheck.org              10\n",
       "content.govdelivery.com     5\n",
       "cnn.com                     4\n",
       "usatoday.com                3\n",
       "time.com                    3\n",
       "buzzfeed.com                2\n",
       "money.cnn.com               2\n",
       "realclearpolitics.com       2\n",
       "bea.gov                     2\n",
       "snopes.com                  2\n",
       "dnainfo.com                 1\n",
       "washingtonmonthly.com       1\n",
       "nbcnews.com                 1\n",
       "transcripts.cnn.com         1\n",
       "warontherocks.com           1\n",
       "markets.on.nytimes.com      1\n",
       "chicagotribune.com          1\n",
       "pbs.org                     1\n",
       "businessinsider.com         1\n",
       "nation.time.com             1\n",
       "thehill.com                 1\n",
       "heritage.org                1\n",
       "palmbeachpost.com           1\n",
       "opensecrets.org             1\n",
       "pewresearch.org             1\n",
       "talkingpointsmemo.com       1\n",
       "public.tableau.com          1\n",
       "mdjonline.com               1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyperlink.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
